{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67109ec7-a6a8-45ae-b73c-c15d50bf0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from torch import nn, optim, save\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from model.dataset import PlantDataset, getTransforms\n",
    "from model.model import TraitDetector\n",
    "from model.train import R2_pred, rmse, train, val_eval\n",
    "from model.utils import set_device\n",
    "\n",
    "\"\"\"Models.\"\"\"\n",
    "\n",
    "import logging\n",
    "import math\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "\n",
    "\n",
    "class TraitDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, train_features):\n",
    "        super(TraitDetector, self).__init__()\n",
    "\n",
    "        # The network is defined as a sequence of operations\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.resnet.requires_grad_(False)\n",
    "        self.resnet.fc = nn.Linear(in_features=2048, out_features=train_features)\n",
    "        # self.resnet.train = lambda x: True\n",
    "        #\n",
    "        # self.resnet.training = False\n",
    "\n",
    "        self.tabular_nn = nn.Sequential(\n",
    "            nn.Linear(in_features=train_features, out_features=train_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_features=train_features, out_features=train_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_features=train_features, out_features=train_features),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.merge_nn = nn.Sequential(\n",
    "            nn.Linear(in_features=2 * train_features, out_features=train_features),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=train_features, out_features=n_classes),\n",
    "        )\n",
    "\n",
    "    # Specify the computations performed on the data\n",
    "    def forward(self, x_image, x_row):\n",
    "        x_image = self.resnet(x_image)\n",
    "        x_row = self.tabular_nn(x_row)\n",
    "\n",
    "        return self.merge_nn(torch.cat((x_image, x_row), axis=1))\n",
    "\n",
    "    def predict(self, x_image, x_row):\n",
    "\n",
    "        output = self.forward(x_image, x_row)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f642d3-5920-4dfc-97ff-5bf26338bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_df = pd.read_csv('data/planttraits2024/train.csv')\n",
    "test_df = pd.read_csv('data/planttraits2024/test.csv')\n",
    "\n",
    "targets = [\"X4_mean\", \"X11_mean\", \"X18_mean\", \"X26_mean\", \"X50_mean\", \"X3112_mean\"]\n",
    "sd = [\"X4_sd\", \"X11_sd\", \"X18_sd\", \"X26_sd\", \"X50_sd\", \"X3112_sd\"]\n",
    "\n",
    "zcored_data = train_df.apply(zscore).drop(axis=1, labels=targets+sd+['id'])\n",
    "mask = (np.abs(zcored_data) > 3).any(axis=1)\n",
    "\n",
    "train_df = train_df[~mask]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "train_variables = train_df.drop(axis=1, labels=targets+sd+['id'])\n",
    "column_variables = train_variables.columns\n",
    "\n",
    "targets_df = train_df[targets+sd+['id']]\n",
    "\n",
    "test_variables = test_df.drop(axis=1, labels=['id'])\n",
    "test_ids = test_df[['id']]\n",
    "\n",
    "transformed_variables = sc.fit_transform(train_variables.values)\n",
    "\n",
    "transformed_train_df = pd.DataFrame(transformed_variables, columns = column_variables)\n",
    "\n",
    "transformed_test = sc.transform(test_variables.values)\n",
    "transformed_test_df = pd.DataFrame(transformed_test, columns=test_variables.columns)\n",
    "\n",
    "merged_transformed_train_df = targets_df.merge(transformed_train_df, left_index=True, right_index=True)\n",
    "merged_transformed_test_df = test_ids.merge(transformed_test_df, left_index=True, right_index=True)\n",
    "\n",
    "merged_transformed_train_df.to_csv('data/planttraits2024/transformed_train_df.csv')\n",
    "merged_transformed_test_df.to_csv('data/planttraits2024/transformed_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b0cbe-aff0-41eb-ae32-ca9f1fcc7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def R2_pred(y_pred, y_true):\n",
    "    SS_residuals = torch.pow(y_pred - y_true, 2).sum()\n",
    "    SS_tot = torch.pow(y_true - y_true.mean(axis=0), 2).sum()\n",
    "    return 1 - SS_residuals / SS_tot\n",
    "\n",
    "\n",
    "def rmse(y_pred, y_true):\n",
    "    return torch.pow(y_pred - y_true, 2).sum()\n",
    "\n",
    "\n",
    "def train(dataloader, model: TraitDetector, loss_fn, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x_image, x_train, y_train = data\n",
    "\n",
    "        x_image = x_image.to(device, dtype=torch.float)\n",
    "        x_train = x_train.to(device, dtype=torch.float)\n",
    "        y_train = y_train.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_pred = model(x_image, x_train)\n",
    "\n",
    "        t_loss = loss_fn(train_pred, y_train)\n",
    "\n",
    "        t_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss.append(t_loss.cpu().detach().numpy())\n",
    "\n",
    "    model.eval()\n",
    "    return np.sqrt(np.sum(train_loss))\n",
    "\n",
    "\n",
    "def val_eval(dataloader, model, loss_fn, device):\n",
    "    y_val_list = []\n",
    "    pred_list = []\n",
    "    val_loss_list = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x_img, x_val, y_val = data\n",
    "\n",
    "        x_img = x_img.to(device, dtype=torch.float)\n",
    "        x_val = x_val.to(device, dtype=torch.float)\n",
    "        y_val = y_val.to(device, dtype=torch.float)\n",
    "\n",
    "        pred = model(x_img, x_val)\n",
    "\n",
    "        val_loss = loss_fn(pred, y_val)\n",
    "\n",
    "        y_val_np = y_val.cpu().detach().numpy().tolist()\n",
    "        pred_np = pred.cpu().detach().numpy().tolist()\n",
    "\n",
    "        val_loss = val_loss.cpu().detach().numpy()\n",
    "\n",
    "        val_loss_list.append(val_loss)\n",
    "        y_val_list.extend(y_val_np)\n",
    "        pred_list.extend(pred_np)\n",
    "    #         print(val_loss_list)\n",
    "    return (\n",
    "        torch.FloatTensor(y_val_list),\n",
    "        torch.FloatTensor(pred_list),\n",
    "        np.sqrt(np.sum(val_loss_list)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1bbb98-3d45-4700-a3ad-93c70bb74d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "SIZE = 512\n",
    "\n",
    "\n",
    "class PlantDataset(Dataset):\n",
    "\n",
    "    targets = [\"X4_mean\", \"X11_mean\", \"X18_mean\", \"X26_mean\", \"X50_mean\", \"X3112_mean\"]\n",
    "    sd = [\"X4_sd\", \"X11_sd\", \"X18_sd\", \"X26_sd\", \"X50_sd\", \"X3112_sd\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_to_csv,\n",
    "        path_to_imgs,\n",
    "        applied_transforms=None,\n",
    "        labeled=False,\n",
    "        num_plants=None,\n",
    "    ):\n",
    "\n",
    "        self.path = pathlib.Path(path_to_imgs)\n",
    "\n",
    "        self.df = pd.read_csv(path_to_csv, dtype={\"id\": str})\n",
    "\n",
    "        self.df.set_index(keys=[\"id\"], drop=True, inplace=True)\n",
    "\n",
    "        if num_plants is not None:\n",
    "            self.df = self.df.iloc[:num_plants]\n",
    "\n",
    "        self.train_columns = self.df.columns[\n",
    "            (~self.df.columns.isin(self.targets)) & (~self.df.columns.isin(self.sd))\n",
    "        ]\n",
    "\n",
    "        if applied_transforms:\n",
    "            self.image_transforms = applied_transforms\n",
    "        else:\n",
    "            self.image_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        self.labeled = labeled\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        plant_id = self.df.index[idx]\n",
    "\n",
    "        image = Image.open(self.path / f\"{plant_id}.jpeg\")\n",
    "        if self.image_transforms:\n",
    "            image = self.image_transforms(image)\n",
    "\n",
    "        if self.labeled:\n",
    "            return (\n",
    "                image,\n",
    "                torch.from_numpy(self.df.loc[plant_id, self.train_columns].values),\n",
    "                torch.from_numpy(self.df.loc[plant_id, self.targets].values),\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            image,\n",
    "            torch.from_numpy(self.df.loc[plant_id, self.train_columns].values),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "\n",
    "def getTransforms():\n",
    "\n",
    "    first_transform = [transforms.ToTensor()]\n",
    "\n",
    "    aug_transforms = [\n",
    "        transforms.RandomResizedCrop(size=SIZE),\n",
    "        transforms.RandomRotation(degrees=180),\n",
    "    ]\n",
    "\n",
    "    preprocessing_transforms = [  # T.ToTensor(),\n",
    "        transforms.Resize(size=SIZE),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    "    train_transformer = transforms.Compose(\n",
    "        first_transform + aug_transforms + preprocessing_transforms\n",
    "    )\n",
    "    val_transformer = transforms.Compose(first_transform + preprocessing_transforms)\n",
    "    return train_transformer, val_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08e324-006b-4dd2-972e-1d664ee14c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook.\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "DEVICE = set_device()\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae62526-7a99-4d98-b7cc-da2206b87a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_est = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_tf, val_tf = getTransforms()\n",
    "\n",
    "dataset = PlantDataset(\n",
    "    \"data/planttraits2024/transformed_train_df.csv\",\n",
    "    \"data/planttraits2024/train_images\",\n",
    "    applied_transforms=train_tf,\n",
    "    labeled=True,\n",
    "    # num_plants=2000\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [0.75, 0.25])\n",
    "# val_dataset = PlantDataset(\"data/planttraits2024/test.csv\", \"data/planttraits2024/test_images\", applied_transforms=val_tf,\n",
    "#                            labeled=True)\n",
    "\n",
    "detector = TraitDetector(n_classes=6, train_features=dataset.train_columns.shape[0])\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(detector.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "# optimizer = optim.SGD(detector.parameters(), lr=0.001, momentum=.5, weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=10, threshold_mode=\"rel\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "device = DEVICE  # our device (single GPU core)\n",
    "model = detector.to(device)  # put model onto the GPU core\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t_loss = train(train_loader, model, loss_fn, optimizer, scheduler, device)\n",
    "    y_true, y_pred, v_loss = val_eval(val_loader, model, loss_fn, device)\n",
    "\n",
    "    r2 = R2_pred(y_pred, y_true)\n",
    "\n",
    "    logger.info(\n",
    "        f\"R2: {r2:2.3f}, train loss: {t_loss:6,.2f}, val_loss: {v_loss:6,.2f}, learning_rate: {optimizer.param_groups[0]['lr']:1.6f}\"\n",
    "    )\n",
    "\n",
    "    r2_est.append(r2)\n",
    "\n",
    "    val_loss.append(v_loss)\n",
    "    train_loss.append(t_loss)\n",
    "    scheduler.step(v_loss)\n",
    "\n",
    "save(model.state_dict(), PATH)\n",
    "\n",
    "# Load:\n",
    "#\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
